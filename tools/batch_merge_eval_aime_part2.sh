#!/bin/bash
#
# æ‰¹é‡æ¨¡å‹åˆå¹¶ä¸ AIME è¯„ä¼°è„šæœ¬ - Part 2 (3 A100 GPU)
# å¯¹åŒ…å« 50/90/105 checkpoint çš„å®éªŒè¿›è¡Œ merge å¹¶è¯„ä¼° AIME24 å’Œ AIME25
# ä¼˜å…ˆçº§é¡ºåº: 105 -> 50 -> 90
# GPU: 0,1,2 (3ä¸ªA100)
#

set -e

# ============ ç¯å¢ƒé…ç½® ============
export CUDA_VISIBLE_DEVICES=0,1,2
export PYTHONPATH=/scratch/jin509/self_RL/Archer2.0:$PYTHONPATH
PYTHON=/scratch/jin509/miniconda3/envs/archer/bin/python

# ============ è·¯å¾„é…ç½® ============
BASE_DIR=/scratch/jin509/self_RL/Archer2.0
OUTPUT_ROOT=${BASE_DIR}/output/ArcherCodeR
DATA_DIR=${BASE_DIR}/data/test

# ============ è¯„ä¼°å‚æ•° ============
n_gpus=3
tp_size=1
n_samples=32           # pass@32
temperature=0.6
top_p=0.95
max_prompt_length=$((1024 * 2))      # 2K prompt
max_response_length=$((1024 * 8))    # 8K response
batch_size=2048

# ============ æ—¥å¿—é…ç½® ============
LOG_DIR=${BASE_DIR}/tools/logs
mkdir -p ${LOG_DIR}
MAIN_LOG=${LOG_DIR}/batch_merge_eval_part2_$(date +%Y%m%d_%H%M%S).log

# ============ å®éªŒåˆ—è¡¨ Part 2 (5ä¸ªå®éªŒ) ============
EXPERIMENTS=(
    "Archer-Intuitor-Qwen2.5-1.5B-2k-8k-batch64-no-kl-n12"
    "Pure-GRPO-Qwen2.5-1.5B-2K-8K-16resp-kl005-v2"
    "Pure-GRPO-Qwen2.5-1.5B-2K-8K-n8-no-kl-v2"
    "Pure-GRPO-Qwen2.5-1.5B-2K-8K-16resp-no-kl-temp0.8-v2"
    "Pure-GRPO-Qwen2.5-1.5B-2K-8K-n12-no-kl-v2"
)

# æŒ‰ä¼˜å…ˆçº§æ’åºçš„ checkpoint steps
STEPS=(105 50 90)

# ============ è¾…åŠ©å‡½æ•° ============

log() {
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a ${MAIN_LOG}
}

# æ£€æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨ actor ç›®å½•
check_checkpoint_exists() {
    local exp_name=$1
    local step=$2
    local ckpt_path="${OUTPUT_ROOT}/${exp_name}/global_step_${step}/actor"
    
    if [ -d "$ckpt_path" ] && [ -f "$ckpt_path/config.json" ]; then
        return 0
    else
        return 1
    fi
}

# æ£€æŸ¥ HF æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
check_hf_model_exists() {
    local exp_name=$1
    local step=$2
    local hf_path="${OUTPUT_ROOT}/${exp_name}/global_step_${step}/actor/hf_model"
    
    if [ -d "$hf_path" ] && [ -f "$hf_path/config.json" ]; then
        return 0
    else
        return 1
    fi
}

# æ£€æŸ¥è¯„ä¼°ç»“æœæ˜¯å¦å·²å­˜åœ¨
check_eval_exists() {
    local exp_name=$1
    local step=$2
    local dataset=$3
    local result_path="${OUTPUT_ROOT}/${exp_name}/global_step_${step}/actor/hf_model/output/${dataset}.parquet"
    
    if [ -f "$result_path" ]; then
        return 0
    else
        return 1
    fi
}

# åˆå¹¶æ¨¡å‹
merge_model() {
    local exp_name=$1
    local step=$2
    local ckpt_path="${OUTPUT_ROOT}/${exp_name}/global_step_${step}/actor"
    local hf_path="${ckpt_path}/hf_model"
    
    log "ğŸ”§ å¼€å§‹åˆå¹¶æ¨¡å‹: ${exp_name} step ${step}"
    
    # æ£€æŸ¥æ˜¯å¦å·²åˆå¹¶
    if check_hf_model_exists "$exp_name" "$step"; then
        log "âœ“ HFæ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡åˆå¹¶: ${hf_path}"
        return 0
    fi
    
    # æ‰§è¡Œåˆå¹¶
    local start_time=$(date +%s)
    
    $PYTHON -m tools.model_merge merge \
        --backend fsdp \
        --local_dir "${ckpt_path}" \
        --target_dir "${hf_path}" 2>&1 | tee -a ${MAIN_LOG}
    
    local exit_code=${PIPESTATUS[0]}
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $exit_code -eq 0 ] && [ -f "${hf_path}/config.json" ]; then
        log "âœ“ æ¨¡å‹åˆå¹¶æˆåŠŸ (è€—æ—¶: ${duration}s): ${exp_name} step ${step}"
        return 0
    else
        log "âœ— æ¨¡å‹åˆå¹¶å¤±è´¥: ${exp_name} step ${step}"
        # æ¸…ç†å¤±è´¥çš„åˆå¹¶ç»“æœ
        [ -d "${hf_path}" ] && rm -rf "${hf_path}"
        return 1
    fi
}

# è¿è¡Œè¯„ä¼°
run_eval() {
    local exp_name=$1
    local step=$2
    local dataset=$3
    local model_path="${OUTPUT_ROOT}/${exp_name}/global_step_${step}/actor/hf_model"
    local output_dir="${model_path}/output"
    
    log "ğŸ“Š å¼€å§‹è¯„ä¼°: ${exp_name} step ${step} - ${dataset}"
    
    # æ£€æŸ¥æ˜¯å¦å·²è¯„ä¼°
    if check_eval_exists "$exp_name" "$step" "$dataset"; then
        log "âœ“ è¯„ä¼°ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡: ${output_dir}/${dataset}.parquet"
        return 0
    fi
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "${output_dir}"
    
    local start_time=$(date +%s)
    
    $PYTHON -m verl.trainer.main_generation \
        trainer.nnodes=1 \
        trainer.n_gpus_per_node=${n_gpus} \
        +trainer.project_name=ArcherCodeR_Eval \
        +trainer.experiment_name=${exp_name} \
        +trainer.task_name=${dataset} \
        +trainer.global_step=${step} \
        +trainer.use_wandb=False \
        model.path=${model_path} \
        data.path=${DATA_DIR}/${dataset}.parquet \
        data.output_path=${output_dir}/${dataset}.parquet \
        data.batch_size=${batch_size} \
        data.n_samples=${n_samples} \
        rollout.name=vllm \
        rollout.gpu_memory_utilization=0.9 \
        rollout.enforce_eager=False \
        rollout.free_cache_engine=False \
        rollout.disable_log_stats=False \
        rollout.tensor_model_parallel_size=${tp_size} \
        rollout.temperature=${temperature} \
        rollout.top_k=-1 \
        rollout.top_p=${top_p} \
        rollout.prompt_length=${max_prompt_length} \
        rollout.response_length=${max_response_length} \
        rollout.max_num_batched_tokens=$((max_prompt_length + max_response_length)) \
        2>&1 | tee -a ${MAIN_LOG}
    
    local exit_code=${PIPESTATUS[0]}
    local end_time=$(date +%s)
    local duration=$((end_time - start_time))
    
    if [ $exit_code -eq 0 ] && [ -f "${output_dir}/${dataset}.parquet" ]; then
        log "âœ“ è¯„ä¼°å®Œæˆ (è€—æ—¶: ${duration}s): ${exp_name} step ${step} - ${dataset}"
        return 0
    else
        log "âœ— è¯„ä¼°å¤±è´¥: ${exp_name} step ${step} - ${dataset}"
        return 1
    fi
}

# å¤„ç†å•ä¸ª checkpoint
process_checkpoint() {
    local exp_name=$1
    local step=$2
    
    log "============================================================"
    log "å¤„ç†: ${exp_name} - global_step_${step}"
    log "============================================================"
    
    # æ£€æŸ¥ checkpoint æ˜¯å¦å­˜åœ¨
    if ! check_checkpoint_exists "$exp_name" "$step"; then
        log "âš  Checkpoint ä¸å­˜åœ¨ï¼Œè·³è¿‡: ${exp_name} step ${step}"
        return 0
    fi
    
    # 1. åˆå¹¶æ¨¡å‹
    if ! merge_model "$exp_name" "$step"; then
        log "âš  åˆå¹¶å¤±è´¥ï¼Œè·³è¿‡è¯„ä¼°: ${exp_name} step ${step}"
        return 1
    fi
    
    # 2. è¯„ä¼° AIME24
    run_eval "$exp_name" "$step" "aime2024"
    
    # 3. è¯„ä¼° AIME25
    run_eval "$exp_name" "$step" "aime2025"
    
    log "âœ“ å®Œæˆå¤„ç†: ${exp_name} - global_step_${step}"
}

# ============ ä¸»æµç¨‹ ============

main() {
    log "============================================================"
    log "æ‰¹é‡æ¨¡å‹åˆå¹¶ä¸ AIME è¯„ä¼°è„šæœ¬ - Part 2 (3 A100)"
    log "============================================================"
    log "å®éªŒæ•°é‡: ${#EXPERIMENTS[@]}"
    log "Checkpoint steps: ${STEPS[*]}"
    log "GPU: 0,1,2 (${n_gpus} GPUs)"
    log "n_samples: ${n_samples} (pass@32)"
    log "batch_size: ${batch_size}"
    log "max_response_length: ${max_response_length}"
    log "============================================================"
    
    # ç»Ÿè®¡è®¡æ•°
    local total_tasks=0
    local completed_tasks=0
    local failed_tasks=0
    
    # æŒ‰ä¼˜å…ˆçº§é¡ºåºå¤„ç†: 105 -> 50 -> 90
    for step in "${STEPS[@]}"; do
        log ""
        log "########################################################"
        log "å¼€å§‹å¤„ç† Step ${step} (æ‰€æœ‰å®éªŒ)"
        log "########################################################"
        
        for exp_name in "${EXPERIMENTS[@]}"; do
            ((total_tasks+=1))
            
            if process_checkpoint "$exp_name" "$step"; then
                ((completed_tasks+=1))
            else
                ((failed_tasks+=1))
            fi
            
            log ""
        done
    done
    
    # æ‰“å°ç»Ÿè®¡
    log "============================================================"
    log "ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ (Part 2)!"
    log "============================================================"
    log "æ€»ä»»åŠ¡æ•°: ${total_tasks}"
    log "æˆåŠŸ: ${completed_tasks}"
    log "å¤±è´¥: ${failed_tasks}"
    log "æ—¥å¿—æ–‡ä»¶: ${MAIN_LOG}"
    log "============================================================"
}

# è¿è¡Œä¸»æµç¨‹
cd ${BASE_DIR}
main

